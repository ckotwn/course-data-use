[multipage-level=2]
== Data Processing
[NOTE.objectives]
In the data processing module, you will familiarize yourself with the concept of fit-for purpose datasets and some commonly used data filters that you may want to consider for creating your own fit-for-purpose dataset. 

=== Fit-for-purpose data
Correctness and Consistency are two ways of documenting data errors and are measures of data quality. 
These are measures of how well the data gatherer was able to capture the true value being investigated. 
The nature of GBIF's data publication workflow means that the correctness and consistency of the data can vary dependent on the data publishers and the source of the data.
Knowing these properties of the data you have, will help you to understand the ways in which you can and cannot clean, validate and process the data.

* Correctness (Accuracy) - closeness of measured values, observations or estimates to the real or true value e.g. has the species been identified correctly or the collection locality been identified correctly i.e.  “How close the recorded values is to the actual value”.

image::img/web/Correctness.png[align=center,width=640,height=360]

In the diagram it’s how close is the shot to the centre of the target.

* Consistency (Precision) - level of resolution of the data e.g. precision of coordinates, taxonomic determination i.e. “How often you get it right”

image::img/web/Consistency.png[align=center,width=640,height=360]

In the diagram it is how close the shots are together irrespective of how near they are to the centre of the target.

For most analyses you want highly accurate data although the level of precision may vary dependent on your analysis.
GBIF can help you to determine the accuracy and precision of the data through, for example, filters and issue flags, however, you must always double-check!

=== Data Processing Tools
As outlined in 'Software Tools'you should use tools appropriate for you for processing your data post donwload. 
While GBIF filters will allow for some data processing, it is highly recommended that you additional data processing including a data visualisation step. 
Tools that can be used for this are: 
* Spreadsheet editing software e.g. Excel, Google Sheets (smaller datasets)
* OpenRefine
* R packages and scripts eg CoordinateCleaner, scrubr and biogeo (automated data processing)
* Geographical Information Systems (GIS) 

=== Common Data Filters
GBIF issue flags ⚐
GBIF has around 50 “issue” flags. These are usually some missing values from the publisher or some other problem. In a download, they can be found in the issue column of a download. 
Two additional issues which might be interesting for users: 
•	TAXON_MATCH_HIGHERRANK
•	TAXON_MATCH_FUZZY
Blog post discussing issue flags in detail: https://data-blog.gbif.org/post/issues-and-flags/
GBIF has around 50 issues flags. 
We have seen a few of them the default geospatial issues. Aside from the default geospatial issues, most of these flags are not very interesting for users. They are usually more useful for data publishers who have left a field blank or made an error during publishing. 
Two issues flags which still might be sometimes useful are: 
•	Taxon match higherrank 
•	The record can be matched to the GBIF taxonomic backbone at a higher rank, but not with the scientific name given.
Reasons include:
•	- The name is new, and not available in the taxonomic datasets yet
•	- The name is missing in the backbone’s taxonomic sources for others reasons
•	- Formatting or spelling of the scientific name caused interpretation errors
•	Taxon match fuzzy
•	Matching to the taxonomic backbone can only be done using a fuzzy, non exact match.

Taxon Keys
If you are accessing GBIF-mediated data programatically as opposed to via the website, taxon keys provide an effective way for defining searches based on taxonomy. 
Scientific names can be messy. So it may make sense to sort out the species by their unique taxon keys provided during the indexation of the dataset by GIBF. 
Taxon keys are issued at the species, genus family, order, phylum and kingdom level. Unique identifiers are issued to accepted names with synonyms of those accepted names issued the same identifier.  
Takon keys allow for discerning between In the previous GBIF API and the version of rgbif that wrapped that API, you could search the equivalent of this function with a species name, which was convenient. 
However, names are messy right. So it sorta makes sense to sort out the species key numbers you want exactly, and then get your occurrence data with this function. 
GBIF has added a parameter scientificName to allow searches by scientific names in this function - which includes synonym taxa. 
Note: that if you do use the scientificName parameter, we will check internally that it's not a synonym of an accepted name, and if it is, we'll search on the accepted name. 
If you want to force searching by a synonym do so by finding the GBIF identifier first with any name_* functions, then pass that ID to the taxonKey parameter.

Almost always you will want to post-process your GBIF download in some way to fit your needs. 
Here I take you through some common data quality filters. 
Sometimes you will have to make difficult judgement calls for your particular use-case. 
Whenever you are dealing with thousands-millions of records, you will never quite know the true quality of the source data. 
It is important to keep in mind that you are always just mitigating data quality issues, not eliminating them. 

== Default Geospatial Issues

GBIF removes common geospatial issues by default if you choose to have data with a location.

The following things will be removed:

*Zero Coordinates- Coordinates are exactly (0,0) or what is sometimes called "null island". Zero-zero coordinate is a very common geospatial issue. GBIF removes (0,0) when hasgeospatialissue is set to FALSE.  
*Country coordinate mis-match - Data publishers will often supply GBIF with a country code (US,TW,SE,JP…). GBIF uses the two letter system. 
https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2. When a point does not fall within the country’s polygon or EEZ, but says that it should occur within the country, it gets flagged as having “country coordinate mis-match” and will be removed if data are filtered for locations.
*Coordinate invalid - If GBIF is unable to interpret the coordinates i.e. the coordinates .
*Coordinate invalid - The coordinates are outside of the range for decimal lat/lon values ((-90,90), (-180,180)).

== Absence records

Sometimes data publishers will include absence records (where they verify that a species in not present). Most of users don’t want these records.
GBIF now has a field for classifying a record as present or absent. 
99% of users will want to have only presence records, so no GBIF gives it to you as the default. 
If you want to make sure you only got presence records, you can include this filter. 

== Fossils and Living Specimens

GBIF has Fossils and Living Specimens (usually a plant inside a botanical garden or sometimes and animal in a zoo).  Most users do not want fossils or plants in botanical gardens. This filter will remove some of these cases. This will not remove all such cases, since some publishers will not fill in the basis of record field correctly. 
establishmentMeans
dwc:establishmentMeans : The process by which the biological individual(s) represented in the Occurrence became established at the location.
Unfortunately not used very often.
This field could allow you to remove records that are not naturally established. Publishers to not fill in this field very often, but there are cases where removing “MANAGED” records will remove zoo records.

== Old Records

GBIF has many museum records that might be older than what is desired for some studies.

== Uncertain location 

Often you will want to be sure that the coordinates give a certain location and are not really 1000s of km away from where the organism was observed or collected. There are two fields (corrdinate precision and coordinateUncertaintyInMeters you get with a SIMPLE CSV download that you can use to filter by “uncertainty”. These fields are not used very often by publishers who feel that their records are fairly certain. But in fact it would be very helpful for users, if all publishers tried to fill in one of these fields. 

I recommend not filtering out missing values, since the value is often not filled in by publishers if they think the occurrence is fairly certain (from a GPS). 
There are a few “fake” values for coordinate uncertainty that you should be aware of. These values are errors produced by geocoding software and do not represent real uncertainty values. In the case of 301, the uncertainty is often much-much greater than 301 and actually represents a country centroid.

Points along the equator or prime meridian

Some publishers consider zero and NULL to be equivalent, empty latitude and longitude end up being plotted along these two lines.

== Country centroids

Country centroids are where the observation is pinned to center of the country instead of being closer to where the animal, plant, or microbe … was observed or recorded.  Country centroids are usually records that have been retrospectively given a lat-lon value based on a textual description of where the original record was located. So if the record simple says “Brazil”, some publishers will put the record in the center of Brazil. Similarly if the record simply says “Texas”, “Paris” … the record will go in the center of those regions. This is almost exclusively a feature of museum data (PRESERVED_SPECIMEN), but it can also happen with other types of records as well. 
Geocoding software uses gazetteers. A gazetteer is a geographical dictionary or directory used in conjunction with a map or atlas.

CoordinateCleaner is an R package for “cleaning up” GBIF occurrences. 
There are a few very helpful functions there especially for removing country centroids.
CoordinateCleaner is especially helpful for removing country centroids. 

== Remove duplicates

For your application it might be important to remove duplicate records.

=== Advanced filtering

There are other things to consider when post processing GBIF data, such.  
Here are some additional things you might want to do to your data. These things are little bit more complex and involve more judgement calls, so I leave them out of the main cleaning pipeline script. 

== Outliers
I have found the DBSCAN to be an effective way to detect points that might be outliers. 

== Metagenomics
Metagenomics datasets sample the environment for DNA and then match the samples against an existing reference database. Especially with non-microorganisms these matches can often be incorrect or suspicious. GBIF has changed its processing so this typically is not a large problem anymore. 

Currently, there is not a great way for filtering for only metagenomics datasets. 

== outside native ranges
== gridded datasets
Most publishers of gridded datasets actually fill in one of the following columns: coordinateuncertaintyinmeters, coordinateprecision, footprintwkt
So filtering by these columns can be a good way to remove gridded datasets.
GBIF has an experimental API for identifying datasets which exhibit a certain about of "griddyness". You can read more here
== automated identifications


